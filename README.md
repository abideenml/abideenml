I design, build, and operate machine learning systems that serve customers at scale. Currently, I'm an LLMs Consultant helping companies/labs/startups to seamlessly integrate LLMs into their infrastructure. 

### üî¨ Recent OS Projects

<!-- writing starts -->
* [llm.pth](https://github.com/abideenml/llm.pth) - Hackable implementations of Autoregressive models (Llama, mixtral, gemma, deepseek), Research papers (cope, yarn, mod, mome, mla) and techniques (sft, dpo, kto, ipo) in Pytorch.
* [LightAgents](https://github.com/abideenml/LightAgents) - A wrapper free Agents library with RAG, function calling, json mode, telemetry and multi-layer memory.
* [llama3.cuda](https://github.com/abideenml/llama3.cuda) - llama3.cuda is an implementation of Llama 3.1 in pure C/CUDA. Consists of Swiglu, RoPE, CSE, RMSNorm and GQA kernels.

### üíª Recent Work Projects

<!-- writing starts -->
* [Elemental Compute](https://www.linkedin.com/company/elemental-compute-limited/) - Implemented a self-optimizing multimodal pipeline with RAG, Agentic workflow, and open-source AI using LLM-as-a-Judge and Mixture of Agents. Managed 30+ GPUs for multi-node inference of the entire multimodal pipeline consisting of LLama-3.1 70B, Phi-3-medium-128k-instruct, Llava-next-8b, and SDXL-Lightning.
* [John Snow Labs](https://www.linkedin.com/company/johnsnowlabs) - Released a series of JSL-MedX 3B, 7B, 8B, and 70B LLMs in the Healthcare domain. JSL-MedX models are ranked No. 1 on the Open Medical Leaderboard across all Param variants.
* [QueryLoopAi](https://www.linkedin.com/company/queryloopai/) - Pre-trained a 500M SLM from scratch on a carefully curated high-quality 15B tokens synthetic dataset. Created the entire training and evaluation pipeline along with managing training on 8xA100s. Created Kendrick, a mixture of experts model with 32k experts and Multi-latent head attention.

### üìù Recent Writing

<!-- writing starts -->
* [Coding Deepseek-V2 from Scratch in PyTorch](https://medium.com/@zaiinn440/coding-deepseek-v2-from-scratch-in-pytorch-06dd89917067) - Thur, 20 Jul 2024
* [MHA vs MQA vs GQA vs MLA](https://medium.com/@zaiinn440/mha-vs-mqa-vs-gqa-vs-mla-c6cf8285bbec) - Mon, 13 Jul 2024
* [Linear Rope vs NTK vs YaRN vs CoPE](https://medium.com/@zaiinn440/linear-rope-vs-ntk-vs-yarn-vs-cope-d33587ddfd35) - Mon, 13 Jul 2024
* [MoE vs Dense vs Hybrid LLM architectures](https://wandb.ai/zaiinn440/hybridMoe/reports/MoE-vs-Dense-vs-Hybrid-LLM-Architectures--Vmlldzo3NzYwNzAw?utm_campaign=Blog+&utm_source=linkedin&utm_medium=social&utm_content=LLMArchitectures) - Tues, 2 May 2024
* [Multi-GPU Training of 70B LLM with Deepspeed and FSDP+Qlora](https://medium.com/@zaiinn440/multi-gpu-training-of-70b-llm-with-deepspeed-and-fsdp-qlora-cb738a2a2229) - Thur, 14 Mar 2024
* [Complete Roadmap For Learning Diffusion Models (Prereqs, DDPM, Stable Diffusion, Dreambooth, and More!)](https://medium.com/ai-in-plain-english/complete-roadmap-for-learning-diffusion-models-prereqs-ddpm-stable-diffusion-dreambooth-and-a15941767180) - Tue, 26 Sep 2023



<!-- writing ends -->

View the archives (42 posts) @ [zain.com](https://medium.com/@zaiinn440).


---

[![Linkedin Follow](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/zaiinulabideen/)[![Medium Follow](https://img.shields.io/badge/Medium-12100E?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@zaiinn440)[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/aMtqaxvH)[![Twitter](https://img.shields.io/badge/X-000000?style=for-the-badge&logo=x&logoColor=white)](https://twitter.com/zaynismm)[![Substack](https://img.shields.io/badge/Substack-%23006f5c.svg?style=for-the-badge&logo=substack&logoColor=FF6719)](https://rethinkai.substack.com/)

